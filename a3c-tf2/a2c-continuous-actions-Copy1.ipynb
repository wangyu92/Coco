{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow2.0\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_DEFAULT_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpel Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.action_dim = 1\n",
    "        self.action_low = 0\n",
    "        self.action_high = 10\n",
    "    \n",
    "    def reset(self):\n",
    "#         self.state = np.random.randint(100, size=(1)) + 0.1\n",
    "        self.state = np.array([1.0])\n",
    "        return self.state\n",
    "        \n",
    "    def obs(self):\n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        next_state = self.state.copy()\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        if self.state[0] > 100:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        elif abs(self.state[0] - action) < 0.5:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = True\n",
    "            \n",
    "        self.state = next_state\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n",
      "(array([1.]), -1, True)\n",
      "(array([1.]), -1, True)\n"
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "print(env.reset())\n",
    "print(env.step(100))\n",
    "print(env.step(7.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C with Continuous Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__('mlp_policy')\n",
    "        \n",
    "        # actor\n",
    "        self.hidden_p1 = keras.layers.Dense(40, activation='elu')\n",
    "        self.hidden_p2 = keras.layers.Dense(40, activation='elu')\n",
    "        self.mu = keras.layers.Dense(1, activation='tanh')\n",
    "        self.sigma = keras.layers.Dense(1, activation='softplus')\n",
    "        \n",
    "        # critic\n",
    "        self.hidden_v1 = keras.layers.Dense(400, activation='elu')\n",
    "        self.hidden_v2 = keras.layers.Dense(400, activation='elu')\n",
    "        self.value = keras.layers.Dense(1, name='value')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs is a numpy array, convert to tensor\n",
    "        x = tf.convert_to_tensor(inputs, dtype=tf.float32)\n",
    "        \n",
    "        # actor\n",
    "        hidden_p = self.hidden_p1(x)\n",
    "        hidden_p = self.hidden_p2(hidden_p)\n",
    "        mu = self.mu(hidden_p)\n",
    "        sigma = self.sigma(hidden_p)\n",
    "        self.norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        action_tf_var = tf.squeeze(self.norm_dist.sample(1), axis=0)\n",
    "        action_tf_var = tf.clip_by_value(action_tf_var, env.action_low, env.action_high)\n",
    "        \n",
    "        # critic\n",
    "        hidden_v = self.hidden_v1(x)\n",
    "        hidden_v = self.hidden_v2(hidden_v)\n",
    "        out_value = self.value(hidden_v)\n",
    "    \n",
    "        return action_tf_var, out_value\n",
    "    \n",
    "    def action_value(self, obs):\n",
    "        policy, value = self.predict(obs)\n",
    "\n",
    "        return np.squeeze(policy, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build our stochastic policy function, estimated by the fully-connected network below. The network input is the state and output are two scalar functions, mu and delta, which are used as the mean and standard deviation of a Gaussian (normal) distribution. We will choose our actions by smapling from this distribution. The stochastic policy provides some dgree of bulit-in exploration mechanism, since the network initialization will cause a non-zero sigma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.] [-2.1459281]\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "a, b = model.action_value(np.array([[44.]]))\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def test(self, env):\n",
    "        state, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(state[None, :])\n",
    "            state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "        return ep_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습이 되지 않았을 때의 성능 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0 %\n"
     ]
    }
   ],
   "source": [
    "agent = A2CAgent(model)\n",
    "iter_cnt = 100\n",
    "succ_cnt = 0\n",
    "for _ in range(100):\n",
    "    rewards_sum = agent.test(env)\n",
    "    if rewards_sum == 10:\n",
    "        succ_cnt += 1\n",
    "print(succ_cnt / 100 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss / Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.params = {\n",
    "            'value': 0.5,\n",
    "            'entropy': 0.001\n",
    "        }\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizer.Adam(lr=0.007),\n",
    "            loss=[self._policy_loss, self._value_loss]\n",
    "        )\n",
    "        \n",
    "    def test(self, env):\n",
    "        state, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(state[None, :])\n",
    "            state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "        return ep_reward\n",
    "    \n",
    "    def _value_loss(self, returns, value):\n",
    "        return self.params['value'] * keras.losses.mean_squared_error(returns, value)\n",
    "    \n",
    "    def _policy_loss(self, action, delta):\n",
    "        return -tf.math.log(model.norm_dist.prob(action) + 1e-5) * delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Trainning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.params = {\n",
    "            'value': 0.5,\n",
    "            'entropy': 0.001,\n",
    "            'gamma':0.99\n",
    "        }\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.RMSprop(lr=0.01),\n",
    "            loss=[self._policy_loss, self._value_loss],\n",
    "            run_eagerly=True\n",
    "        )\n",
    "        \n",
    "    def test(self, env):\n",
    "        state, done, ep_reward = env.reset(), False, 0\n",
    "        while not done:\n",
    "            action, _ = self.model.action_value(state[None, :])\n",
    "            state, reward, done = env.step(action)\n",
    "            ep_reward += reward\n",
    "        return ep_reward\n",
    "    \n",
    "    def train(self, env, batch_size=32, updates=10000):\n",
    "        # storage helpers for a single batch of data\n",
    "        actions = np.empty((batch_size,), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, batch_size))\n",
    "        states = np.empty((batch_size, ) + env.state.shape)\n",
    "        \n",
    "        # training loop: collect samples, send to optimize, repeat updates times\n",
    "        ep_rews = [0.0]\n",
    "        next_state = env.reset()\n",
    "        for update in range(updates):\n",
    "            if update % 10 == 0:\n",
    "                print(str(update) + 'epochs')\n",
    "                \n",
    "            # batch_size 만큼 시도해보면서 데이터를 모음\n",
    "            for step in range(batch_size):\n",
    "                states[step] = next_state.copy()\n",
    "                actions[step], values[step] = self.model.action_value(np.expand_dims(next_state, axis=0))\n",
    "                next_state, rewards[step], dones[step] = env.step(actions[step])\n",
    "\n",
    "                ep_rews[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rews.append(0.0)\n",
    "                    next_state = env.reset()\n",
    "\n",
    "            \n",
    "            _, next_value = self.model.action_value(next_state[None, :])\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "            # a trick to input actions and advantages through same API\n",
    "            acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "            # performs a full training step on the collected batch\n",
    "            # note: no need to mess around with gradients, Keras API handles it\n",
    "            losses = self.model.train_on_batch(states, [advs, returns])\n",
    "        return ep_rews\n",
    "    \n",
    "    def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "        # next_value is the bootstrap value estimate of a future state (the critic)\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # returns are calculated as discounted sum of future rewards\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.params['gamma'] * returns[t+1] * (1-dones[t])\n",
    "        returns = returns[:-1]\n",
    "        # advantages are returns - baseline, value estimates in our case\n",
    "        advantages = returns - values\n",
    "        return returns, advantages\n",
    "    \n",
    "    def _value_loss(self, returns, value):\n",
    "        return keras.losses.mean_squared_error(returns, value)\n",
    "    \n",
    "    def _policy_loss(self, advantages, action):\n",
    "        logprobs = self.model.norm_dist.log_prob(action)\n",
    "        entropy = self.model.norm_dist.entropy()\n",
    "        return tf.math.reduce_mean(-logprobs * advantages - 0.01 * entropy)\n",
    "#         return -tf.math.log(model.norm_dist.prob(action) + 1e-5) * advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0epochs\n",
      "10epochs\n",
      "20epochs\n",
      "30epochs\n",
      "40epochs\n",
      "50epochs\n",
      "60epochs\n",
      "70epochs\n",
      "80epochs\n",
      "90epochs\n",
      "Training Runtime: 110.074565 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st_time = time.time()\n",
    "agent = A2CAgent(model)\n",
    "env = Env()\n",
    "env.reset()\n",
    "rewards_history = agent.train(env)\n",
    "print(\"Training Runtime: %f sec\" % (time.time() - st_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % succecss\n"
     ]
    }
   ],
   "source": [
    "iter_cnt = 100\n",
    "succ_cnt = 0\n",
    "\n",
    "for _ in range(iter_cnt):\n",
    "    rewards = agent.test(env)\n",
    "    if rewards == 10:\n",
    "        succ_cnt += 1\n",
    "\n",
    "print(\"%d %% succecss\" % (succ_cnt / iter_cnt * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.]], dtype=float32), array([[-1.1572691]], dtype=float32)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array([1.,])\n",
    "model.predict(np.expand_dims(state, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Neg_2:0\", shape=(None, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lr_actor = 0.00001\n",
    "lr_critic = 0.00056\n",
    "\n",
    "state = np.array([1., 1.])\n",
    "\n",
    "# instantiate state-value function & policy network\n",
    "action_tf_var, V = model.action_value(np.expand_dims(state, axis=0))\n",
    "\n",
    "# define actor (policy) loss function\n",
    "norm_dist = model.norm_dist\n",
    "loss_actor = -tf.math.log(norm_dist.prob(action))\n",
    "\n",
    "print(loss_actor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
