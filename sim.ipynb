{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=''\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requriement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99 # discount factor\n",
    "A_DIM = 6\n",
    "ENTROPY_WEIGHT = 0.5\n",
    "ENTROPY_EPS = 1e-6\n",
    "S_INFO = 4\n",
    "\n",
    "\n",
    "class ActorNetwork():\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the distribution\n",
    "    of all actions.\n",
    "    \n",
    "    [Parameters]\n",
    "    sess: tf.Sesssion()\n",
    "    state_dim : state dimension인가?\n",
    "    action_dim : action dimension인가?\n",
    "    learning_rate : Learning rate.\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.lr_rate = learning_rate\n",
    "\n",
    "        # Create the actor network\n",
    "        self.inputs, self.out = self.create_actor_network()\n",
    "\n",
    "        # Get all network parameters\n",
    "        # Trainable variable에 소속된 variable을 가져온다.\n",
    "        self.network_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='actor')\n",
    "\n",
    "        # Set all network parameters\n",
    "        self.input_network_params = []\n",
    "        # 가져온 Trainable parameter들을 self.input_network_params에 placeholder로 추가\n",
    "        for param in self.network_params:\n",
    "            self.input_network_params.append(tf.placeholder(tf.float32, shape=param.get_shape()))\n",
    "        \n",
    "        # Place holder를 추가한 파라미터를 기존 파라미터에 assign함\n",
    "        self.set_network_params_op = []\n",
    "        for idx, param in enumerate(self.input_network_params):\n",
    "            self.set_network_params_op.append(self.network_params[idx].assign(param))\n",
    "\n",
    "        # Selected action, 0-1 vector\n",
    "        # action을 placeholder로 지정하는 것으로 보임\n",
    "        self.acts = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        # critic network에 의해서 제공되는 gradient를 placeholder로 지정\n",
    "        self.act_grad_weights = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Compute the objective (log action_vector and entropy)\n",
    "        \n",
    "        self.obj = tf.reduce_sum(tf.multiply(\n",
    "                       tf.log(tf.reduce_sum(tf.multiply(self.out, self.acts),\n",
    "                                            reduction_indices=1, keep_dims=True)),\n",
    "                       -self.act_grad_weights)) \\\n",
    "                   + ENTROPY_WEIGHT * tf.reduce_sum(tf.multiply(self.out,\n",
    "                                                           tf.log(self.out + ENTROPY_EPS)))\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.actor_gradients = tf.gradients(self.obj, self.network_params)\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.RMSPropOptimizer(self.lr_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        \"\"\"\n",
    "        네트워크를 생성해주는 함수인 것으로 보임.\n",
    "        \"\"\"\n",
    "        # variable_scope을 actor로 지정함.\n",
    "        with tf.variable_scope('actor'):\n",
    "            \n",
    "            # state의 dimension을 넘겨줘서 input레이어를 생성.\n",
    "            # placeholder에 해당됨\n",
    "            # rank가 3인 tensor가 반환됨\n",
    "            inputs = tflearn.input_data(shape=[None, self.s_dim[0], self.s_dim[1]])\n",
    "            \n",
    "            # input에 연결될 네트워크를 여러 조각으로 나눔\n",
    "            split_0 = tflearn.fully_connected(inputs[:, 0:1, -1], 128, activation='relu') # input 하나\n",
    "            split_1 = tflearn.fully_connected(inputs[:, 1:2, -1], 128, activation='relu') # input 하나\n",
    "            split_2 = tflearn.conv_1d(inputs[:, 2:3, :], 128, 4, activation='relu') # vector input\n",
    "            split_3 = tflearn.conv_1d(inputs[:, 3:4, :], 128, 4, activation='relu') # vector input\n",
    "            split_4 = tflearn.conv_1d(inputs[:, 4:5, :A_DIM], 128, 4, activation='relu') # vector, action 갯수 만큼\n",
    "            split_5 = tflearn.fully_connected(inputs[:, 4:5, -1], 128, activation='relu') # input 하나\n",
    "\n",
    "            # flatten 한 이유가 뭐지?\n",
    "            split_2_flat = tflearn.flatten(split_2)\n",
    "            split_3_flat = tflearn.flatten(split_3)\n",
    "            split_4_flat = tflearn.flatten(split_4)\n",
    "\n",
    "            merge_net = tflearn.merge([split_0, split_1, split_2_flat, split_3_flat, split_4_flat, split_5], 'concat')\n",
    "\n",
    "            dense_net_0 = tflearn.fully_connected(merge_net, 128, activation='relu')\n",
    "            out = tflearn.fully_connected(dense_net_0, self.a_dim, activation='softmax')\n",
    "\n",
    "            return inputs, out\n",
    "        \n",
    "    def train(self, inputs, acts, act_grad_weights):\n",
    "\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.acts: acts,\n",
    "            self.act_grad_weights: act_grad_weights\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def get_gradients(self, inputs, acts, act_grad_weights):\n",
    "        return self.sess.run(self.actor_gradients, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.acts: acts,\n",
    "            self.act_grad_weights: act_grad_weights\n",
    "        })\n",
    "\n",
    "    def apply_gradients(self, actor_gradients):\n",
    "        return self.sess.run(self.optimize, feed_dict={\n",
    "            i: d for i, d in zip(self.actor_gradients, actor_gradients)\n",
    "        })\n",
    "\n",
    "    def get_network_params(self):\n",
    "        return self.sess.run(self.network_params)\n",
    "\n",
    "    def set_network_params(self, input_network_params):\n",
    "        self.sess.run(self.set_network_params_op, feed_dict={\n",
    "            i: d for i, d in zip(self.input_network_params, input_network_params)\n",
    "        })\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is V(s).\n",
    "    On policy: the action must be obtained from the output of the Actor network.\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, state_dim, learning_rate):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.lr_rate = learning_rate\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.out = self.create_critic_network()\n",
    "\n",
    "        # Get all network parameters\n",
    "        self.network_params = \\\n",
    "            tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='critic')\n",
    "\n",
    "        # Set all network parameters\n",
    "        self.input_network_params = []\n",
    "        for param in self.network_params:\n",
    "            self.input_network_params.append(\n",
    "                tf.placeholder(tf.float32, shape=param.get_shape()))\n",
    "        self.set_network_params_op = []\n",
    "        for idx, param in enumerate(self.input_network_params):\n",
    "            self.set_network_params_op.append(self.network_params[idx].assign(param))\n",
    "\n",
    "        # Network target V(s)\n",
    "        self.td_target = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Temporal Difference, will also be weights for actor_gradients\n",
    "        self.td = tf.subtract(self.td_target, self.out)\n",
    "\n",
    "        # Mean square error\n",
    "        self.loss = tflearn.mean_square(self.td_target, self.out)\n",
    "\n",
    "        # Compute critic gradient\n",
    "        self.critic_gradients = tf.gradients(self.loss, self.network_params)\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.RMSPropOptimizer(self.lr_rate).\\\n",
    "            apply_gradients(zip(self.critic_gradients, self.network_params))\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        with tf.variable_scope('critic'):\n",
    "            inputs = tflearn.input_data(shape=[None, self.s_dim[0], self.s_dim[1]])\n",
    "\n",
    "            split_0 = tflearn.fully_connected(inputs[:, 0:1, -1], 128, activation='relu')\n",
    "            split_1 = tflearn.fully_connected(inputs[:, 1:2, -1], 128, activation='relu')\n",
    "            split_2 = tflearn.conv_1d(inputs[:, 2:3, :], 128, 4, activation='relu')\n",
    "            split_3 = tflearn.conv_1d(inputs[:, 3:4, :], 128, 4, activation='relu')\n",
    "            split_4 = tflearn.conv_1d(inputs[:, 4:5, :A_DIM], 128, 4, activation='relu')\n",
    "            split_5 = tflearn.fully_connected(inputs[:, 4:5, -1], 128, activation='relu')\n",
    "\n",
    "            split_2_flat = tflearn.flatten(split_2)\n",
    "            split_3_flat = tflearn.flatten(split_3)\n",
    "            split_4_flat = tflearn.flatten(split_4)\n",
    "\n",
    "            merge_net = tflearn.merge([split_0, split_1, split_2_flat, split_3_flat, split_4_flat, split_5], 'concat')\n",
    "\n",
    "            dense_net_0 = tflearn.fully_connected(merge_net, 128, activation='relu')\n",
    "            out = tflearn.fully_connected(dense_net_0, 1, activation='linear')\n",
    "\n",
    "            return inputs, out\n",
    "\n",
    "    def train(self, inputs, td_target):\n",
    "        return self.sess.run([self.loss, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.td_target: td_target\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def get_td(self, inputs, td_target):\n",
    "        return self.sess.run(self.td, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.td_target: td_target\n",
    "        })\n",
    "\n",
    "    def get_gradients(self, inputs, td_target):\n",
    "        return self.sess.run(self.critic_gradients, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.td_target: td_target\n",
    "        })\n",
    "\n",
    "    def apply_gradients(self, critic_gradients):\n",
    "        return self.sess.run(self.optimize, feed_dict={\n",
    "            i: d for i, d in zip(self.critic_gradients, critic_gradients)\n",
    "        })\n",
    "\n",
    "    def get_network_params(self):\n",
    "        return self.sess.run(self.network_params)\n",
    "\n",
    "    def set_network_params(self, input_network_params):\n",
    "        self.sess.run(self.set_network_params_op, feed_dict={\n",
    "            i: d for i, d in zip(self.input_network_params, input_network_params)\n",
    "        })\n",
    "\n",
    "\n",
    "def compute_gradients(s_batch, a_batch, r_batch, terminal, actor, critic):\n",
    "    \"\"\"\n",
    "    batch of s, a, r is from samples in a sequence\n",
    "    the format is in np.array([batch_size, s/a/r_dim])\n",
    "    terminal is True when sequence ends as a terminal state\n",
    "    \"\"\"\n",
    "    assert s_batch.shape[0] == a_batch.shape[0]\n",
    "    assert s_batch.shape[0] == r_batch.shape[0]\n",
    "    ba_size = s_batch.shape[0]\n",
    "\n",
    "    v_batch = critic.predict(s_batch)\n",
    "\n",
    "    R_batch = np.zeros(r_batch.shape)\n",
    "\n",
    "    if terminal:\n",
    "        R_batch[-1, 0] = 0  # terminal state\n",
    "    else:\n",
    "        R_batch[-1, 0] = v_batch[-1, 0]  # boot strap from last state\n",
    "\n",
    "    for t in reversed(xrange(ba_size - 1)):\n",
    "        R_batch[t, 0] = r_batch[t] + GAMMA * R_batch[t + 1, 0]\n",
    "\n",
    "    td_batch = R_batch - v_batch\n",
    "\n",
    "    actor_gradients = actor.get_gradients(s_batch, a_batch, td_batch)\n",
    "    critic_gradients = critic.get_gradients(s_batch, R_batch)\n",
    "\n",
    "    return actor_gradients, critic_gradients, td_batch\n",
    "\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Given vector x, computes a vector y such that\n",
    "    y[i] = x[i] + gamma * x[i+1] + gamma^2 x[i+2] + ...\n",
    "    \"\"\"\n",
    "    out = np.zeros(len(x))\n",
    "    out[-1] = x[-1]\n",
    "    for i in reversed(xrange(len(x)-1)):\n",
    "        out[i] = x[i] + gamma*out[i+1]\n",
    "    assert x.ndim >= 1\n",
    "    # More efficient version:\n",
    "    # scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_entropy(x):\n",
    "    \"\"\"\n",
    "    Given vector x, computes the entropy\n",
    "    H(x) = - sum( p * log(p))\n",
    "    \"\"\"\n",
    "    H = 0.0\n",
    "    for i in xrange(len(x)):\n",
    "        if 0 < x[i] < 1:\n",
    "            H -= x[i] * np.log(x[i])\n",
    "    return H\n",
    "\n",
    "\n",
    "def build_summaries():\n",
    "    td_loss = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"TD_loss\", td_loss)\n",
    "    eps_total_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Eps_total_reward\", eps_total_reward)\n",
    "    avg_entropy = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Avg_entropy\", avg_entropy)\n",
    "\n",
    "    summary_vars = [td_loss, eps_total_reward, avg_entropy]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Envinronment\n",
    "- 클러스터, 클라이언트 갯수를 랜덤으로 생성하고 초기화\n",
    "- 각 클러스터에 스트림을 제공하면 Utility metric을 계산해서 알려줌\n",
    "- dataset으로부터 trace를 가져와서 CH의 REMB를 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[ 5 16  1 18 28 29 26 17 18 27]\n",
      "185\n"
     ]
    }
   ],
   "source": [
    "RANDOM_SEED = None\n",
    "RANDOM_SEED = 10\n",
    "\n",
    "class Environment:\n",
    "    \"\"\"\n",
    "    WebRTC를 이용한 [One server -- Many] 스트리밍 아키텍쳐를 구현함.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -- class method --\n",
    "    def get_trace_files(trace_dir):\n",
    "        \"\"\"\n",
    "        trace 파일을 리스트로 만들어서 반환함.\n",
    "        \n",
    "        Arguments:\n",
    "        trace_dir - trace 파일이 있는 디렉토레 경로\n",
    "        \n",
    "        Return: trace 파일의 경로가 담긴 리스트\n",
    "        \"\"\"\n",
    "        \n",
    "        filepaths = []\n",
    "        for filename in os.listdir(trace_dir):\n",
    "            if filename[0] != \".\":\n",
    "                trace_path = trace_dir + filename\n",
    "                filepaths.append(trace_path)\n",
    "        return filepaths\n",
    "    \n",
    "    # -- instance method --\n",
    "    def __init__(self,\n",
    "                 random_seed=RANDOM_SEED,\n",
    "                 num_of_cluster_low=1,\n",
    "                 num_of_cluster_high=100,\n",
    "                 num_of_client_low=1,\n",
    "                 num_of_client_high=30,\n",
    "                 trace_year_month=None,\n",
    "                 trace_dir=\"./dataset/cooked/\",\n",
    "                 trace_type=\"youtube.com\"\n",
    "                ):\n",
    "        \n",
    "        # -- random seed --\n",
    "        self.random_seed = random_seed\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        # -- network configuration --\n",
    "        # 클러스터 갯수를 랜덤으로 지정\n",
    "        self.num_of_cluster = np.random.randint(low=num_of_cluster_low, high=num_of_cluster_high)\n",
    "        # 각 클러스터 안에 포함된 클라이언트의 수\n",
    "        self.num_of_client_in_each_cluster = np.random.randint(low=num_of_client_low, high=num_of_client_high, size=self.num_of_cluster)\n",
    "        \n",
    "        # -- trace --\n",
    "        # cooked directory에 있는 모든 트레이스 파일\n",
    "        self.trace_files_all = Environment.get_trace_files(trace_dir)\n",
    "        # trace file 중에서 클러스터 갯수만큼 랜덤으로 trace들을 선택함\n",
    "        self.selected_traces = self.get_random_traces(self.trace_files_all, self.num_of_cluster)\n",
    "        # get_remb_of_cluster_head() 함수를 부를 때 마다 bandwidth를 변경하기 위해서 존재함\n",
    "        self.trace_iterator = 0\n",
    "        \n",
    "    def get_random_traces(self, traces, size):\n",
    "        \"\"\"\n",
    "        trace들과 size를 지정하면 trace들에서 랜덤으로 size 갯수만큼 trace를 뽑음\n",
    "        \"\"\"\n",
    "        \n",
    "        random_trace_files = np.random.choice(traces, size=size, replace=False)\n",
    "        \n",
    "        traces = []\n",
    "        for path in random_trace_files:\n",
    "            bws = []\n",
    "            with open(path, 'rb') as f:\n",
    "                for line in f:\n",
    "                    line = line.decode()\n",
    "                    throughput = int(line)\n",
    "                    bws.append(throughput * 8 / 1000000) # Convert from Byte to Mbps\n",
    "            traces.append(bws)\n",
    "                    \n",
    "        return traces\n",
    "    \n",
    "    def get_remb_of_cluster_head(self):\n",
    "        \"\"\"\n",
    "        부를 때 마다 다음 트레이스를 차례대로 가져옴\n",
    "        \"\"\"\n",
    "        \n",
    "        idx = self.trace_iterator\n",
    "        remb_list = []\n",
    "        for i in range(self.num_of_cluster):\n",
    "            idx_r = idx % len(self.selected_traces[i])\n",
    "            bw = self.selected_traces[i][idx_r]\n",
    "            remb_list.append(bw)\n",
    "        self.trace_iterator += 1\n",
    "        return remb_list\n",
    "    \n",
    "    def set_bitrate_of_streams(bitrates_of_streams):\n",
    "        \"\"\"\n",
    "        각 stream을 설정해주면 Utility를 계산해서 되돌려줌.\n",
    "        \"\"\"\n",
    "        utility = 0\n",
    "        \n",
    "        \n",
    "        return utility\n",
    "        \n",
    "    def p(self):\n",
    "        print(self.num_of_cluster)\n",
    "        print(self.num_of_client_in_each_cluster)\n",
    "        print(sum(self.num_of_client_in_each_cluster))\n",
    "        \n",
    "        \n",
    "env = Environment()\n",
    "env.p()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.371344, 5.083928, 2.20748, 5.174328, 5.41904, 2.449376, 5.273504, 5.257552, 6.052704, 2.324288]\n"
     ]
    }
   ],
   "source": [
    "a = env.get_remb_of_cluster_head()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_trace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-31c15ce32ef3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-298-31c15ce32ef3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mcoordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mall_cooked_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_cooked_bw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_TRACES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0magents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_AGENTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_trace' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wangyu/miniconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/wangyu/miniconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-298-31c15ce32ef3>\", line 79, in central_agent\n",
      "    actor = a3c.ActorNetwork(sess,\n",
      "NameError: name 'a3c' is not defined\n"
     ]
    }
   ],
   "source": [
    "S_INFO = 6  # bit_rate, buffer_size, next_chunk_size, bandwidth_measurement(throughput and time), chunk_til_video_end\n",
    "S_LEN = 8  # take how many frames in the past\n",
    "A_DIM = 6\n",
    "ACTOR_LR_RATE = 0.0001\n",
    "CRITIC_LR_RATE = 0.001\n",
    "NUM_AGENTS = 16\n",
    "TRAIN_SEQ_LEN = 100  # take as a train batch\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "VIDEO_BIT_RATE = [300,750,1200,1850,2850,4300]  # Kbps\n",
    "HD_REWARD = [1, 2, 3, 12, 15, 20]\n",
    "BUFFER_NORM_FACTOR = 10.0\n",
    "CHUNK_TIL_VIDEO_END_CAP = 48.0\n",
    "M_IN_K = 1000.0\n",
    "REBUF_PENALTY = 4.3  # 1 sec rebuffering -> 3 Mbps\n",
    "SMOOTH_PENALTY = 1\n",
    "DEFAULT_QUALITY = 1  # default video quality without agent\n",
    "RANDOM_SEED = 42\n",
    "RAND_RANGE = 1000\n",
    "SUMMARY_DIR = './results'\n",
    "LOG_FILE = './results/log'\n",
    "TEST_LOG_FOLDER = './test_results/'\n",
    "TRAIN_TRACES = './cooked_traces/'\n",
    "# NN_MODEL = './results/pretrain_linear_reward.ckpt'\n",
    "NN_MODEL = None\n",
    "\n",
    "\n",
    "def testing(epoch, nn_model, log_file):\n",
    "    # clean up the test results folder\n",
    "    os.system('rm -r ' + TEST_LOG_FOLDER)\n",
    "    os.system('mkdir ' + TEST_LOG_FOLDER)\n",
    "    \n",
    "    # run test script\n",
    "    os.system('python rl_test.py ' + nn_model)\n",
    "    \n",
    "    # append test performance to the log\n",
    "    rewards = []\n",
    "    test_log_files = os.listdir(TEST_LOG_FOLDER)\n",
    "    for test_log_file in test_log_files:\n",
    "        reward = []\n",
    "        with open(TEST_LOG_FOLDER + test_log_file, 'rb') as f:\n",
    "            for line in f:\n",
    "                parse = line.split()\n",
    "                try:\n",
    "                    reward.append(float(parse[-1]))\n",
    "                except IndexError:\n",
    "                    break\n",
    "        rewards.append(np.sum(reward[1:]))\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "\n",
    "    rewards_min = np.min(rewards)\n",
    "    rewards_5per = np.percentile(rewards, 5)\n",
    "    rewards_mean = np.mean(rewards)\n",
    "    rewards_median = np.percentile(rewards, 50)\n",
    "    rewards_95per = np.percentile(rewards, 95)\n",
    "    rewards_max = np.max(rewards)\n",
    "\n",
    "    log_file.write(str(epoch) + '\\t' +\n",
    "                   str(rewards_min) + '\\t' +\n",
    "                   str(rewards_5per) + '\\t' +\n",
    "                   str(rewards_mean) + '\\t' +\n",
    "                   str(rewards_median) + '\\t' +\n",
    "                   str(rewards_95per) + '\\t' +\n",
    "                   str(rewards_max) + '\\n')\n",
    "    log_file.flush()\n",
    "\n",
    "\n",
    "def central_agent(net_params_queues, exp_queues):\n",
    "\n",
    "    assert len(net_params_queues) == NUM_AGENTS\n",
    "    assert len(exp_queues) == NUM_AGENTS\n",
    "\n",
    "    logging.basicConfig(filename=LOG_FILE + '_central',\n",
    "                        filemode='w',\n",
    "                        level=logging.INFO)\n",
    "\n",
    "    with tf.Session() as sess, open(LOG_FILE + '_test', 'wb') as test_log_file:\n",
    "\n",
    "        actor = a3c.ActorNetwork(sess,\n",
    "                                 state_dim=[S_INFO, S_LEN], action_dim=A_DIM,\n",
    "                                 learning_rate=ACTOR_LR_RATE)\n",
    "        critic = a3c.CriticNetwork(sess,\n",
    "                                   state_dim=[S_INFO, S_LEN],\n",
    "                                   learning_rate=CRITIC_LR_RATE)\n",
    "\n",
    "        summary_ops, summary_vars = a3c.build_summaries()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)  # training monitor\n",
    "        saver = tf.train.Saver()  # save neural net parameters\n",
    "\n",
    "        # restore neural net parameters\n",
    "        nn_model = NN_MODEL\n",
    "        if nn_model is not None:  # nn_model is the path to file\n",
    "            saver.restore(sess, nn_model)\n",
    "            print(\"Model restored.\")\n",
    "\n",
    "        epoch = 0\n",
    "\n",
    "        # assemble experiences from agents, compute the gradients\n",
    "        while True:\n",
    "            # synchronize the network parameters of work agent\n",
    "            actor_net_params = actor.get_network_params()\n",
    "            critic_net_params = critic.get_network_params()\n",
    "            for i in xrange(NUM_AGENTS):\n",
    "                net_params_queues[i].put([actor_net_params, critic_net_params])\n",
    "                # Note: this is synchronous version of the parallel training,\n",
    "                # which is easier to understand and probe. The framework can be\n",
    "                # fairly easily modified to support asynchronous training.\n",
    "                # Some practices of asynchronous training (lock-free SGD at\n",
    "                # its core) are nicely explained in the following two papers:\n",
    "                # https://arxiv.org/abs/1602.01783\n",
    "                # https://arxiv.org/abs/1106.5730\n",
    "\n",
    "            # record average reward and td loss change\n",
    "            # in the experiences from the agents\n",
    "            total_batch_len = 0.0\n",
    "            total_reward = 0.0\n",
    "            total_td_loss = 0.0\n",
    "            total_entropy = 0.0\n",
    "            total_agents = 0.0 \n",
    "\n",
    "            # assemble experiences from the agents\n",
    "            actor_gradient_batch = []\n",
    "            critic_gradient_batch = []\n",
    "\n",
    "            for i in xrange(NUM_AGENTS):\n",
    "                s_batch, a_batch, r_batch, terminal, info = exp_queues[i].get()\n",
    "\n",
    "                actor_gradient, critic_gradient, td_batch = \\\n",
    "                    a3c.compute_gradients(\n",
    "                        s_batch=np.stack(s_batch, axis=0),\n",
    "                        a_batch=np.vstack(a_batch),\n",
    "                        r_batch=np.vstack(r_batch),\n",
    "                        terminal=terminal, actor=actor, critic=critic)\n",
    "\n",
    "                actor_gradient_batch.append(actor_gradient)\n",
    "                critic_gradient_batch.append(critic_gradient)\n",
    "\n",
    "                total_reward += np.sum(r_batch)\n",
    "                total_td_loss += np.sum(td_batch)\n",
    "                total_batch_len += len(r_batch)\n",
    "                total_agents += 1.0\n",
    "                total_entropy += np.sum(info['entropy'])\n",
    "\n",
    "            # compute aggregated gradient\n",
    "            assert NUM_AGENTS == len(actor_gradient_batch)\n",
    "            assert len(actor_gradient_batch) == len(critic_gradient_batch)\n",
    "            # assembled_actor_gradient = actor_gradient_batch[0]\n",
    "            # assembled_critic_gradient = critic_gradient_batch[0]\n",
    "            # for i in xrange(len(actor_gradient_batch) - 1):\n",
    "            #     for j in xrange(len(assembled_actor_gradient)):\n",
    "            #             assembled_actor_gradient[j] += actor_gradient_batch[i][j]\n",
    "            #             assembled_critic_gradient[j] += critic_gradient_batch[i][j]\n",
    "            # actor.apply_gradients(assembled_actor_gradient)\n",
    "            # critic.apply_gradients(assembled_critic_gradient)\n",
    "            for i in xrange(len(actor_gradient_batch)):\n",
    "                actor.apply_gradients(actor_gradient_batch[i])\n",
    "                critic.apply_gradients(critic_gradient_batch[i])\n",
    "\n",
    "            # log training information\n",
    "            epoch += 1\n",
    "            avg_reward = total_reward  / total_agents\n",
    "            avg_td_loss = total_td_loss / total_batch_len\n",
    "            avg_entropy = total_entropy / total_batch_len\n",
    "\n",
    "            logging.info('Epoch: ' + str(epoch) +\n",
    "                         ' TD_loss: ' + str(avg_td_loss) +\n",
    "                         ' Avg_reward: ' + str(avg_reward) +\n",
    "                         ' Avg_entropy: ' + str(avg_entropy))\n",
    "\n",
    "            summary_str = sess.run(summary_ops, feed_dict={\n",
    "                summary_vars[0]: avg_td_loss,\n",
    "                summary_vars[1]: avg_reward,\n",
    "                summary_vars[2]: avg_entropy\n",
    "            })\n",
    "\n",
    "            writer.add_summary(summary_str, epoch)\n",
    "            writer.flush()\n",
    "\n",
    "            if epoch % MODEL_SAVE_INTERVAL == 0:\n",
    "                # Save the neural net parameters to disk.\n",
    "                save_path = saver.save(sess, SUMMARY_DIR + \"/nn_model_ep_\" +\n",
    "                                       str(epoch) + \".ckpt\")\n",
    "                logging.info(\"Model saved in file: \" + save_path)\n",
    "                testing(epoch, \n",
    "                    SUMMARY_DIR + \"/nn_model_ep_\" + str(epoch) + \".ckpt\", \n",
    "                    test_log_file)\n",
    "\n",
    "\n",
    "def agent(agent_id, all_cooked_time, all_cooked_bw, net_params_queue, exp_queue):\n",
    "\n",
    "    net_env = env.Environment(all_cooked_time=all_cooked_time,\n",
    "                              all_cooked_bw=all_cooked_bw,\n",
    "                              random_seed=agent_id)\n",
    "\n",
    "    with tf.Session() as sess, open(LOG_FILE + '_agent_' + str(agent_id), 'wb') as log_file:\n",
    "        actor = a3c.ActorNetwork(sess,\n",
    "                                 state_dim=[S_INFO, S_LEN], action_dim=A_DIM,\n",
    "                                 learning_rate=ACTOR_LR_RATE)\n",
    "        critic = a3c.CriticNetwork(sess,\n",
    "                                   state_dim=[S_INFO, S_LEN],\n",
    "                                   learning_rate=CRITIC_LR_RATE)\n",
    "\n",
    "        # initial synchronization of the network parameters from the coordinator\n",
    "        actor_net_params, critic_net_params = net_params_queue.get()\n",
    "        actor.set_network_params(actor_net_params)\n",
    "        critic.set_network_params(critic_net_params)\n",
    "\n",
    "        last_bit_rate = DEFAULT_QUALITY\n",
    "        bit_rate = DEFAULT_QUALITY\n",
    "\n",
    "        action_vec = np.zeros(A_DIM)\n",
    "        action_vec[bit_rate] = 1\n",
    "\n",
    "        s_batch = [np.zeros((S_INFO, S_LEN))]\n",
    "        a_batch = [action_vec]\n",
    "        r_batch = []\n",
    "        entropy_record = []\n",
    "\n",
    "        time_stamp = 0\n",
    "        while True:  # experience video streaming forever\n",
    "\n",
    "            # the action is from the last decision\n",
    "            # this is to make the framework similar to the real\n",
    "            delay, sleep_time, buffer_size, rebuf, \\\n",
    "            video_chunk_size, next_video_chunk_sizes, \\\n",
    "            end_of_video, video_chunk_remain = \\\n",
    "                net_env.get_video_chunk(bit_rate)\n",
    "\n",
    "            time_stamp += delay  # in ms\n",
    "            time_stamp += sleep_time  # in ms\n",
    "\n",
    "            # -- linear reward --\n",
    "            # reward is video quality - rebuffer penalty - smoothness\n",
    "            reward = VIDEO_BIT_RATE[bit_rate] / M_IN_K \\\n",
    "                     - REBUF_PENALTY * rebuf \\\n",
    "                     - SMOOTH_PENALTY * np.abs(VIDEO_BIT_RATE[bit_rate] -\n",
    "                                               VIDEO_BIT_RATE[last_bit_rate]) / M_IN_K\n",
    "\n",
    "            # -- log scale reward --\n",
    "            # log_bit_rate = np.log(VIDEO_BIT_RATE[bit_rate] / float(VIDEO_BIT_RATE[-1]))\n",
    "            # log_last_bit_rate = np.log(VIDEO_BIT_RATE[last_bit_rate] / float(VIDEO_BIT_RATE[-1]))\n",
    "\n",
    "            # reward = log_bit_rate \\\n",
    "            #          - REBUF_PENALTY * rebuf \\\n",
    "            #          - SMOOTH_PENALTY * np.abs(log_bit_rate - log_last_bit_rate)\n",
    "\n",
    "            # -- HD reward --\n",
    "            # reward = HD_REWARD[bit_rate] \\\n",
    "            #          - REBUF_PENALTY * rebuf \\\n",
    "            #          - SMOOTH_PENALTY * np.abs(HD_REWARD[bit_rate] - HD_REWARD[last_bit_rate])\n",
    "\n",
    "            r_batch.append(reward)\n",
    "\n",
    "            last_bit_rate = bit_rate\n",
    "\n",
    "            # retrieve previous state\n",
    "            if len(s_batch) == 0:\n",
    "                state = [np.zeros((S_INFO, S_LEN))]\n",
    "            else:\n",
    "                state = np.array(s_batch[-1], copy=True)\n",
    "\n",
    "            # dequeue history record\n",
    "            state = np.roll(state, -1, axis=1)\n",
    "\n",
    "            # this should be S_INFO number of terms\n",
    "            state[0, -1] = VIDEO_BIT_RATE[bit_rate] / float(np.max(VIDEO_BIT_RATE))  # last quality\n",
    "            state[1, -1] = buffer_size / BUFFER_NORM_FACTOR  # 10 sec\n",
    "            state[2, -1] = float(video_chunk_size) / float(delay) / M_IN_K  # kilo byte / ms\n",
    "            state[3, -1] = float(delay) / M_IN_K / BUFFER_NORM_FACTOR  # 10 sec\n",
    "            state[4, :A_DIM] = np.array(next_video_chunk_sizes) / M_IN_K / M_IN_K  # mega byte\n",
    "            state[5, -1] = np.minimum(video_chunk_remain, CHUNK_TIL_VIDEO_END_CAP) / float(CHUNK_TIL_VIDEO_END_CAP)\n",
    "\n",
    "            # compute action probability vector\n",
    "            action_prob = actor.predict(np.reshape(state, (1, S_INFO, S_LEN)))\n",
    "            action_cumsum = np.cumsum(action_prob)\n",
    "            bit_rate = (action_cumsum > np.random.randint(1, RAND_RANGE) / float(RAND_RANGE)).argmax()\n",
    "            # Note: we need to discretize the probability into 1/RAND_RANGE steps,\n",
    "            # because there is an intrinsic discrepancy in passing single state and batch states\n",
    "\n",
    "            entropy_record.append(a3c.compute_entropy(action_prob[0]))\n",
    "\n",
    "            # log time_stamp, bit_rate, buffer_size, reward\n",
    "            log_file.write(str(time_stamp) + '\\t' +\n",
    "                           str(VIDEO_BIT_RATE[bit_rate]) + '\\t' +\n",
    "                           str(buffer_size) + '\\t' +\n",
    "                           str(rebuf) + '\\t' +\n",
    "                           str(video_chunk_size) + '\\t' +\n",
    "                           str(delay) + '\\t' +\n",
    "                           str(reward) + '\\n')\n",
    "            log_file.flush()\n",
    "\n",
    "            # report experience to the coordinator\n",
    "            if len(r_batch) >= TRAIN_SEQ_LEN or end_of_video:\n",
    "                exp_queue.put([s_batch[1:],  # ignore the first chuck\n",
    "                               a_batch[1:],  # since we don't have the\n",
    "                               r_batch[1:],  # control over it\n",
    "                               end_of_video,\n",
    "                               {'entropy': entropy_record}])\n",
    "\n",
    "                # synchronize the network parameters from the coordinator\n",
    "                actor_net_params, critic_net_params = net_params_queue.get()\n",
    "                actor.set_network_params(actor_net_params)\n",
    "                critic.set_network_params(critic_net_params)\n",
    "\n",
    "                del s_batch[:]\n",
    "                del a_batch[:]\n",
    "                del r_batch[:]\n",
    "                del entropy_record[:]\n",
    "\n",
    "                log_file.write('\\n')  # so that in the log we know where video ends\n",
    "\n",
    "            # store the state and action into batches\n",
    "            if end_of_video:\n",
    "                last_bit_rate = DEFAULT_QUALITY\n",
    "                bit_rate = DEFAULT_QUALITY  # use the default action here\n",
    "\n",
    "                action_vec = np.zeros(A_DIM)\n",
    "                action_vec[bit_rate] = 1\n",
    "\n",
    "                s_batch.append(np.zeros((S_INFO, S_LEN)))\n",
    "                a_batch.append(action_vec)\n",
    "\n",
    "            else:\n",
    "                s_batch.append(state)\n",
    "\n",
    "                action_vec = np.zeros(A_DIM)\n",
    "                action_vec[bit_rate] = 1\n",
    "                a_batch.append(action_vec)\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    assert len(VIDEO_BIT_RATE) == A_DIM\n",
    "\n",
    "    # create result directory\n",
    "    if not os.path.exists(SUMMARY_DIR):\n",
    "        os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "    # inter-process communication queues\n",
    "    net_params_queues = []\n",
    "    exp_queues = []\n",
    "    for i in range(NUM_AGENTS):\n",
    "        net_params_queues.append(mp.Queue(1))\n",
    "        exp_queues.append(mp.Queue(1))\n",
    "\n",
    "    # create a coordinator and multiple agent processes\n",
    "    # (note: threading is not desirable due to python GIL)\n",
    "    coordinator = mp.Process(target=central_agent,\n",
    "                             args=(net_params_queues, exp_queues))\n",
    "    coordinator.start()\n",
    "\n",
    "    all_cooked_time, all_cooked_bw, _ = load_trace.load_trace(TRAIN_TRACES)\n",
    "    agents = []\n",
    "    for i in xrange(NUM_AGENTS):\n",
    "        agents.append(mp.Process(target=agent,\n",
    "                                 args=(i, all_cooked_time, all_cooked_bw,\n",
    "                                       net_params_queues[i],\n",
    "                                       exp_queues[i])))\n",
    "    for i in xrange(NUM_AGENTS):\n",
    "        agents[i].start()\n",
    "\n",
    "    # wait unit training is done\n",
    "    coordinator.join()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
