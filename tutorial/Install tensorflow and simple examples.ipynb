{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow 용어\n",
    "\n",
    "1. Operation\n",
    "    - 그래프 상의 노드는 op로 불림\n",
    "    - 하나 이상의 텐서를 받을 수 있음\n",
    "    - 계산을 수행하고 결과로 하나 이상의 텐서로 반환할 수 있음\n",
    "2. Tensor\n",
    "    - 내부적으로 모든 데이터는 텐서를 통해서 표현됨\n",
    "    - 텐서는 일종의 다차원 배열인데, 그래프 내의 오퍼레이션 간에는 텐서만이 전달됨\n",
    "3. Session\n",
    "    - 그래프를 실행하기 위해서는 세션 객체가 필요함\n",
    "    - 세션은 오퍼레이션의 실행 환경을 캡슐화한 것임\n",
    "4. Variable\n",
    "    - 변수는 그래프의 실행 시, 파라미터를 저장하고 갱신하는데 사용됨\n",
    "    - 메모리 상에서 텐서를 저장하는 버퍼 역할을 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 간단한 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, Tensorflow!'\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, Tensorflow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print(sess.run(a + b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 텐서플로우 데이터형\n",
    "1. Placeholder\n",
    "2. Variable\n",
    "3. Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_10:0\", shape=(), dtype=string)\n",
      "Tensor(\"Add_4:0\", shape=(), dtype=int32)\n",
      "b'Hello, Tensorflow!'\n",
      "[10, 32, 42]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.constant 상수\n",
    "hello = tf.constant('Hello, Tensorflow!')\n",
    "print(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "c = tf.add(a, b)\n",
    "print(c)\n",
    "\n",
    "# 위에서 변수와 수식들을 정의했지만 실행이 정의한 시점에서 실행되는 것은 아님\n",
    "# 다음처럼 Session 객체와 run 메소드를 사용할 때 계산됨\n",
    "# 따라서 모델을 구성하는 것과 실행하는 것을 분리하여 프로그램을 깔끔하게 작성할 수 있음\n",
    "# 그래프를 실행할 세션을 구성함\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))\n",
    "print(sess.run([a, b, c]))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_10:0\", shape=(?, 3), dtype=float32)\n",
      "=== x_data ===\n",
      "[[1, 2, 3], [4, 5, 6]]\n",
      "=== W ===\n",
      "[[-0.11711758 -0.11156276]\n",
      " [ 0.8473318   0.4438187 ]\n",
      " [-0.30991387 -0.02535409]]\n",
      "=== b ===\n",
      "[[2.4240634 ]\n",
      " [0.58892447]]\n",
      "=== expr ===\n",
      "[[3.071868  3.124076 ]\n",
      " [2.49763   2.2096424]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.placeholder 계산을 실행할 때 입력 값을 받는 변수로 사용함\n",
    "# None은 크기가 정해지지 않았음을 의미함\n",
    "X = tf.placeholder(tf.float32, [None, 3])\n",
    "print(X)\n",
    "\n",
    "# X 플레이스홀더에 넣을 값입니다.\n",
    "# 플레이스홀더에서 설정한 것처럼 두번째 차원의 요소 갯수는 3개임\n",
    "x_data = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]\n",
    "\n",
    "# tf.Variable: 그래프를 계산하면서 최적화할 변수들임. 이 값이 신경망을 좌우하는 값들임.\n",
    "# tf.random_normal: 각 변수들의 초기값을 정규분포 랜덤 값으로 초기화함.\n",
    "W = tf.Variable(tf.random_normal([3, 2]))\n",
    "b = tf.Variable(tf.random_normal([2, 1]))\n",
    "\n",
    "# 입력값과 변수들을 계산할 수식을 작성함\n",
    "# tf.matmul 처럼 mat*로 되어 있는 함수로 행렬 계산을 수행함\n",
    "expr = tf.matmul(X, W) + b\n",
    "\n",
    "sess = tf.Session()\n",
    "# 위에서 설정한 Variable들의 값들을 초기화하기 위해\n",
    "# 처음에 tf.global_variables_initializer를 한번 실행해야함\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(\"=== x_data ===\")\n",
    "print(x_data)\n",
    "print(\"=== W ===\")\n",
    "print(sess.run(W))\n",
    "print(\"=== b ===\")\n",
    "print(sess.run(b))\n",
    "print(\"=== expr ===\")\n",
    "# expr 수식에는 X라는 입력값이 필요함\n",
    "# 따라서 expr 실행 시에는 이 변수에 대한 실제 입력값을 다음처럼 넣어줘야함\n",
    "\n",
    "print(sess.run(expr, feed_dict={X: x_data}))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"X_26:0\", dtype=float32)\n",
      "Tensor(\"Y_26:0\", dtype=float32)\n",
      "\n",
      "=== Test ===\n",
      "X: 5, Y: [10.]\n",
      "X: 2.5, Y: [5.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_data = [1, 2, 3]\n",
    "y_data = [2, 4, 6]\n",
    "\n",
    "# W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "# b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "\n",
    "W = tf.Variable(tf.random_normal([1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "# name: 나중에 텐서보드 등으로 값의 변화를 추적하거나 살펴보기 쉽게 하기 위해 이름을 붙여줌\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "# X와 Y의 상관 관계를 분석하기 위한 가설 수식(Hypothesis)을 작성함\n",
    "# y = W * x + b\n",
    "# W와 X가 행렬이 아니므로 tf.matmul이 아니라 기본 곱셈 기호를 사용했음\n",
    "hypothesis = W * X + b\n",
    "\n",
    "# Loss function을 작성\n",
    "# mean(h - Y)^2 : 예측값과 실제값의 거리를 cost 함수로 정의함\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "# 텐서플로우에 기본적으로 포함되어 있는 함수를 이용해 gradient descent 를 수행함\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# cost function을 최소화하는 것이 최종 목표\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# 세션을 생성하고 초기화함\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 최적화를 100번 수행함\n",
    "    for step in range(100000):\n",
    "        # sess.run을 통해 train_op와 cost 그래프를 계산함\n",
    "        # 이 때, 가설 수식에 넣어야할 실제값을 feed_dict을 통해 전달함\n",
    "        _, cost_val = sess.run([train_op, cost], feed_dict={X: x_data, Y: y_data})\n",
    "        \n",
    "#         print(step, cost_val, sess.run(W), sess.run(b))\n",
    "        \n",
    "    # 최적화가 완료된 모델에 테스트 값을 넣고 결과가 잘 나오는지 확인해봄\n",
    "    print(\"\\n=== Test ===\")\n",
    "    print(\"X: 5, Y:\", sess.run(hypothesis, feed_dict={X: 5}))\n",
    "    print(\"X: 2.5, Y:\", sess.run(hypothesis, feed_dict={X: 2.5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값: [2 2 2 2 2 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 33.33\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 0],\n",
    "        [1, 1],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 함\n",
    "y_data = np.array(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "##########\n",
    "### NN ###\n",
    "##########\n",
    "\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 신경망은 2차원적으로 [입력층(특성), 출력층(레이블)]->[2, 3]으로 정의함\n",
    "W = tf.Variable(tf.random_uniform([2, 3], -1., 1.))\n",
    "\n",
    "# 편향은 각 레이어의 아웃풋 갯수로 설정\n",
    "# 편향은 아웃풋의 갯수, 즉 최종 결과값의 분류 갯수인 3으로 설정\n",
    "b = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 가중치 W와 편향 b를 적용\n",
    "L = tf.add(tf.matmul(X, W), b)\n",
    "# 가중치와 편향을 이용해 계산한 결과 값에\n",
    "# 텐서플로우에서 기본적으로 제공하는 활성화 함수인 ReLU 함수를 적용\n",
    "L = tf.nn.relu(L)\n",
    "\n",
    "# 마지막으로 softmax 함수를 이용하여 출력값을 사용하기 쉽게 만듦\n",
    "# softmax 함수는 다음처럼 결과값을 전체합이 1인 확률로 만들어주는 함수\n",
    "model = tf.nn.softmax(L)\n",
    "\n",
    "# 최적화를 위해서 cost function을 작성\n",
    "# 각 개별 결과에 대한 합을 구한 뒤 평균을 내는 방식을 사용함\n",
    "# 전체 합이 아닌 개별 결과를 구한 뒤 평균을 내는 방식을 사용하기 위해 axis 옵션을 사용\n",
    "# axis 옵션이 없으면 -1.09 처럼 총합인 스칼라값으로 출력됨\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(model), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "\n",
    "##########\n",
    "# 학습\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(10000):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "#     if(step + 1) % 10 == 0:\n",
    "#         print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "        \n",
    "##########\n",
    "# 결과확인\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X: x_data}))\n",
    "print(\"실제값:\", sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_corret = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_corret, tf.float32))\n",
    "print(\"정확도: %.2f\" % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deep NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.27881378\n",
      "20 0.08105924\n",
      "30 0.023550145\n",
      "40 0.009695139\n",
      "50 0.005532933\n",
      "60 0.0038555816\n",
      "70 0.0030076497\n",
      "80 0.0024916385\n",
      "90 0.002132315\n",
      "100 0.0018580946\n",
      "예측값: [0 1 2 0 0 2]\n",
      "실제값: [0 1 2 0 0 2]\n",
      "정확도: 100.00\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# [털, 날개]\n",
    "x_data = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [1, 0],\n",
    "        [1, 1],\n",
    "        [0, 0],\n",
    "        [0, 0],\n",
    "        [0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# [기타, 포유류, 조류]\n",
    "# 다음과 같은 형식을 one-hot 형식의 데이터라고 함\n",
    "y_data = np.array(\n",
    "    [\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "##########\n",
    "### NN ###\n",
    "##########\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# 첫번째 가중치의 차원은 [특성, 히든 레이어의 뉴런갯수] -> [2, 10]으로 정의함\n",
    "W1 = tf.Variable(tf.random_uniform([2, 100], -1., 1.))\n",
    "# 두번째 가중치의 차원을 [첫번째 히든레이어의 뉴런 갯수, 분류 갯수] -> [10, 3]으로 설정함\n",
    "W2 = tf.Variable(tf.random_uniform([100, 3], -1., 1.))\n",
    "\n",
    "# 편향을 각 레이어의 아웃풋 갯수로 정함\n",
    "b1 = tf.Variable(tf.zeros([100]))\n",
    "b2 = tf.Variable(tf.zeros([3]))\n",
    "\n",
    "# 신경망의 히든 레이어에 가중치 W1과 편향 b1을 적용함\n",
    "L1 = tf.add(tf.matmul(X, W1), b1)\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "# 최종적인 아웃풋을 계산\n",
    "# 히든레이어에 두번째 가중치 W2와 편향 b2를 적용하여 3개의 출력값을 만들어냄\n",
    "model = tf.add(tf.matmul(L1, W2), b2)\n",
    "\n",
    "# 텐서플로우에서 기볹거으로 제공되는 크로스엔트로피 함수를 이용해\n",
    "# 복잡한 수식을 사용하지 않고도 최적화를 위한 비용 함수를 다음처럼 간단하게 적용할 수 있음\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=model))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "##########\n",
    "# 학습\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(100):\n",
    "    sess.run(train_op, feed_dict={X: x_data, Y: y_data})\n",
    "    \n",
    "    if(step+1) % 10 == 0:\n",
    "        print(step + 1, sess.run(cost, feed_dict={X: x_data, Y: y_data}))\n",
    "        \n",
    "#########\n",
    "# 결과 확인\n",
    "# 0: 기타 1: 포유류, 2: 조류\n",
    "######\n",
    "prediction = tf.argmax(model, 1)\n",
    "target = tf.argmax(Y, 1)\n",
    "print(\"예측값:\", sess.run(prediction, feed_dict={X: x_data}))\n",
    "print(\"실제값:\", sess.run(target, feed_dict={Y: y_data}))\n",
    "\n",
    "is_correct = tf.equal(prediction, target)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"정확도: %.2f\" % sess.run(accuracy * 100, feed_dict={X: x_data, Y: y_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CNN-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 14:15:21.410708 139830860023616 deprecation.py:323] From <ipython-input-81-6115b6ab5014>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0725 14:15:21.412244 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W0725 14:15:21.413583 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W0725 14:15:22.703494 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 14:15:23.080940 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W0725 14:15:23.085375 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 14:15:23.792952 139830860023616 deprecation.py:323] From /home/wangyu/miniconda3/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W0725 14:15:23.949282 139830860023616 deprecation.py:506] From <ipython-input-81-6115b6ab5014>:44: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Avg. cost = 0.362\n",
      "Epoch: 0002 Avg. cost = 0.113\n",
      "Epoch: 0003 Avg. cost = 0.080\n",
      "Epoch: 0004 Avg. cost = 0.064\n",
      "Epoch: 0005 Avg. cost = 0.052\n",
      "Epoch: 0006 Avg. cost = 0.044\n",
      "Epoch: 0007 Avg. cost = 0.038\n",
      "Epoch: 0008 Avg. cost = 0.032\n",
      "Epoch: 0009 Avg. cost = 0.029\n",
      "Epoch: 0010 Avg. cost = 0.026\n",
      "Epoch: 0011 Avg. cost = 0.022\n",
      "Epoch: 0012 Avg. cost = 0.022\n",
      "Epoch: 0013 Avg. cost = 0.020\n",
      "Epoch: 0014 Avg. cost = 0.018\n",
      "Epoch: 0015 Avg. cost = 0.015\n",
      "최적화 완료!\n",
      "정확도: 0.9898\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot= True)\n",
    "\n",
    "################\n",
    "# 신경망 모델 구성\n",
    "\n",
    "# 기존 모델에서는 입력 값을 28x28 하나의 차원으로 구성하였으나,\n",
    "# CNN 모델을 사용하기 위해 2차원 평면과 특성치의 형태를 갖는 구조로 만듦\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 각각의 변수와 레이어는 다음과 같은 형태로 성성됨\n",
    "# W1 [3 3 1 32] -> [3 3]: 커널크기, 1 : 입력값 X의 특성 수, 32:필터 갯수\n",
    "# L1 Conv shape = (?, 28, 28, 32)\n",
    "# Pool -> (?, 14, 14, 32)\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
    "# tf.nn.conv2d를 이용해 한칸씩 움직이는 컨볼루션 레이어를 쉽게 만들 수 있음\n",
    "# padding='SAME'은 커널 슬라이딩시 최외곽에서 한칸 밖으로 더 움직이는 옵션\n",
    "L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "# Pooling 역시 tf.nn.max_pool을 이용하여 쉽게 구성할 수 있음\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# L1 = tf.nn.dropout(L1, keep_prob)\n",
    "\n",
    "# L2 Conv shape=(?, 14, 14, 64)\n",
    "# Pool -> (?, 7, 7, 64)\n",
    "# W2의 [3, 3, 32, 64]에서 32는 L1에서 출력된 W1의 마지막 차원, 필터의 크기\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# L2 = tf.nn.dropout(L2, keep_prob)\n",
    "\n",
    "# FC레이어: 입력값 7x7x64 -> 출력값 256\n",
    "# Fully connected를 위해 직전의 Pool사이즈인 (?, 7, 7, 64)를 참고하여 차원을 줄여줌\n",
    "# Reshape -> (?, 256)\n",
    "W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 256], stddev=0.01))\n",
    "L3 = tf.reshape(L2, [-1, 7 * 7 * 64])\n",
    "L3 = tf.matmul(L3, W3)\n",
    "L3 = tf.nn.relu(L3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "# 최종 출력값 L3 에서의 출력 256개를 입력값으로 받아서 0~9 레이블인 10개의 출력값을 만듭니다.\n",
    "W4 = tf.Variable(tf.random_normal([256, 10], stddev=0.01))\n",
    "model = tf.matmul(L3, W4)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "# 최적화 함수를 RMSPropOptimizer 로 바꿔서 결과를 확인해봅시다.\n",
    "# optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # 이미지 데이터를 CNN 모델을 위한 자료형태인 [28 28 1] 의 형태로 재구성합니다.\n",
    "        batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "\n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                               feed_dict={X: batch_xs,\n",
    "                                          Y: batch_ys,\n",
    "                                          keep_prob: 0.7})\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.3f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료!')\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "######\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy,\n",
    "                        feed_dict={X: mnist.test.images.reshape(-1, 28, 28, 1),\n",
    "                                   Y: mnist.test.labels,\n",
    "                                   keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 구성을 손쉽게 해 주는 유틸리티 모음인 tensorflow.layers 를 사용해봅니다.\n",
    "# 01 - CNN.py 를 재구성한 것이니, 소스를 한 번 비교해보세요.\n",
    "# 이처럼 TensorFlow 에는 간단하게 사용할 수 있는 다양한 함수와 유틸리티들이 매우 많이 마련되어 있습니다.\n",
    "# 다만, 처음에는 기본적인 개념에 익숙히지는 것이 좋으므로 이후에도 가급적 기본 함수들을 이용하도록 하겠습니다.\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# 기본적으로 inputs, outputs size, kernel_size 만 넣어주면\n",
    "# 활성화 함수 적용은 물론, 컨볼루션 신경망을 만들기 위한 나머지 수치들은 알아서 계산해줍니다.\n",
    "# 특히 Weights 를 계산하는데 xavier_initializer 를 쓰고 있는 등,\n",
    "# 크게 신경쓰지 않아도 일반적으로 효율적인 신경망을 만들어줍니다.\n",
    "L1 = tf.layers.conv2d(X, 32, [3, 3], activation=tf.nn.relu)\n",
    "L1 = tf.layers.max_pooling2d(L1, [2, 2], [2, 2])\n",
    "L1 = tf.layers.dropout(L1, 0.7, is_training)\n",
    "\n",
    "L2 = tf.layers.conv2d(L1, 64, [3, 3], activation=tf.nn.relu)\n",
    "L2 = tf.layers.max_pooling2d(L2, [2, 2], [2, 2])\n",
    "L2 = tf.layers.dropout(L2, 0.7, is_training)\n",
    "\n",
    "L3 = tf.contrib.layers.flatten(L2)\n",
    "L3 = tf.layers.dense(L3, 256, activation=tf.nn.relu)\n",
    "L3 = tf.layers.dropout(L3, 0.5, is_training)\n",
    "\n",
    "model = tf.layers.dense(L3, 10, activation=None)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "#########\n",
    "# 신경망 모델 학습\n",
    "######\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "for epoch in range(15):\n",
    "    total_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
    "        _, cost_val = sess.run([optimizer, cost],\n",
    "                               feed_dict={X: batch_xs,\n",
    "                                          Y: batch_ys,\n",
    "                                          is_training: True})\n",
    "        total_cost += cost_val\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'Avg. cost =', '{:.4f}'.format(total_cost / total_batch))\n",
    "\n",
    "print('최적화 완료!')\n",
    "\n",
    "#########\n",
    "# 결과 확인\n",
    "######\n",
    "is_correct = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print('정확도:', sess.run(accuracy,\n",
    "                        feed_dict={X: mnist.test.images.reshape(-1, 28, 28, 1),\n",
    "                                   Y: mnist.test.labels,\n",
    "                                   is_training: False}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Decaying E-greedy - DQN 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final DQN\n",
      "Qs\n",
      "Success Rate:  0.0007\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def one_hot_encoder(state_in):\n",
    "    return np.identity(16)[state_in:state_in + 1]\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "input_size = env.observation_space.n\n",
    "output_size = env.action_space.n\n",
    "learning_rate = 0.7\n",
    "\n",
    "X = tf.placeholder(shape=[1, input_size], dtype=tf.float32)\n",
    "W = tf.Variable(tf.random_uniform([input_size, output_size], 0, 0.01))\n",
    "\n",
    "Qpredict = tf.matmul(X, W)\n",
    "Y = tf.placeholder(shape=[1, output_size], dtype=tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(Y - Qpredict))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "num_episodes = 10000\n",
    "e = 0.2\n",
    "r = 0.9\n",
    "rList = []\n",
    "successRate = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = None\n",
    "        local_loss = []\n",
    "        while not done:\n",
    "            Qs = sess.run(Qpredict, feed_dict={X:one_hot_encoder(state)})\n",
    "            \n",
    "            rand = random.random()\n",
    "            if(rand < e / (i / 50 + 10)):\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Qs)\n",
    "                \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                Qs[0, action] = reward\n",
    "            else:\n",
    "                new_Qs = sess.run(Qpredict, feed_dict={X:one_hot_encoder(new_state)})\n",
    "                Qs[0:action] = reward + r * np.max(new_Qs)\n",
    "            sess.run(train, feed_dict={X:one_hot_encoder(state), Y:Qs})\n",
    "            total_reward += reward\n",
    "            state = new_state\n",
    "        rList.append(total_reward)\n",
    "        successRate.append(sum(rList) / (i + 1))\n",
    "        \n",
    "print(\"Final DQN\")\n",
    "print(\"Qs\")\n",
    "print(\"Success Rate: \", successRate[-1])\n",
    "plt.plot(range(len(rList)), rList)\n",
    "plt.plot(range(len(successRate)), successRate)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
